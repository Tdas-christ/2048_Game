{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObATxgBA7mSZ2Uq53hJF4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tdas-christ/2048_Game/blob/main/2348569_RL_Lab10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Set up the environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "num_states = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "max_episodes = 500\n",
        "max_steps_per_episode = 200\n",
        "\n",
        "# Build a simple policy model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(24, activation=\"relu\", input_shape=(num_states,)),\n",
        "    layers.Dense(num_actions, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "def choose_action(state):\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "    probs = model(state)\n",
        "    action = np.random.choice(num_actions, p=probs.numpy()[0])\n",
        "    return action, probs[0, action]\n",
        "\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    # Normalize returns for stability\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
        "    return returns\n",
        "\n",
        "all_episode_rewards = []\n",
        "\n",
        "for episode in range(max_episodes):\n",
        "    state = env.reset()\n",
        "    states, actions, rewards = [], [], []\n",
        "    with tf.GradientTape() as tape:\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action, action_prob = choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "\n",
        "        # Compute loss: Sum over t of log(pi(a_t|s_t)) * G_t\n",
        "        action_masks = tf.one_hot(actions, num_actions)\n",
        "\n",
        "        states = np.array(states)\n",
        "        logits = model(states, training=True)\n",
        "        log_probs = tf.math.log(tf.reduce_sum(action_masks * logits, axis=1) + 1e-8)\n",
        "\n",
        "        loss = -tf.reduce_mean(log_probs * returns)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    episode_reward = np.sum(rewards)\n",
        "    all_episode_rewards.append(episode_reward)\n",
        "    if (episode + 1) % 50 == 0:\n",
        "        avg_reward = np.mean(all_episode_rewards[-50:])\n",
        "        print(f\"Episode {episode + 1}, Average Reward (last 50 eps): {avg_reward:.2f}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "lzsbuiJ5HINy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(all_episode_rewards, label='Episode Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Policy Gradient Training Progress')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bLiuAa_LHiX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 10\n",
        "smoothed_rewards = [np.mean(all_episode_rewards[i:i+window_size]) for i in range(len(all_episode_rewards)-window_size)]\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(smoothed_rewards, label='Smoothed Episode Reward (Window=10)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Smoothed Reward')\n",
        "plt.title('Smoothed Policy Performance')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NZAQCe3rHjxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example visualization: action probabilities over a grid of \"Cart Position\" and \"Pole Angle\" (simplified)\n",
        "import numpy as np\n",
        "\n",
        "cart_positions = np.linspace(-2.4, 2.4, 30)\n",
        "pole_angles = np.linspace(-0.20944, 0.20944, 30)  # ~12 degrees in radians\n",
        "policy_map = np.zeros((len(cart_positions), len(pole_angles)))\n",
        "\n",
        "for i, cp in enumerate(cart_positions):\n",
        "    for j, pa in enumerate(pole_angles):\n",
        "        # Construct a \"representative\" state with zero velocities\n",
        "        state = np.array([cp, 0.0, pa, 0.0])\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        probs = model(state).numpy()[0]\n",
        "        # Let's store the probability of choosing action 0 (go left)\n",
        "        policy_map[i, j] = probs[0]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(policy_map, origin='lower', cmap='viridis', extent=[pole_angles[0], pole_angles[-1], cart_positions[0], cart_positions[-1]], aspect='auto')\n",
        "plt.colorbar(label='Probability of Choosing Action 0 (Left)')\n",
        "plt.xlabel('Pole Angle')\n",
        "plt.ylabel('Cart Position')\n",
        "plt.title('Policy Action Probability Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D_WE7LgsHw4_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}